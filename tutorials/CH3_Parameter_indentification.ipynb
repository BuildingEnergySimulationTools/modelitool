{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import os\n",
    "\n",
    "TUTORIAL_DIR = Path(os.getcwd()).as_posix()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Notebooks are written for Jupyter and might not display well in Gitlab***\n",
    "\n",
    "\n",
    "# Parameters Identification\n",
    "\n",
    "In the previous chapters we :\n",
    "- Cleaned a dataset of measure to feed a physical model of the test bench\n",
    "- Proposed a physical model of the experimental setup\n",
    "- Identified the relative influence of the material thermal properties\n",
    "\n",
    "We concluded that three parameters had a strong influence on the discrepancy between\n",
    "the temperature measured between two insulation layers and its model prediction.\n",
    ": the insulation thermal conductivity $\\lambda_{etics}$, the coating coefficient\n",
    "of absorption of solar radiation $\\alpha_{coating}$, the thermal resistance $R_{ext}$\n",
    "modeling external surface conductive and convective heat transfers.\n",
    "\n",
    "The last parameter is a model assumption, we choose to discard it.\n",
    "We will only identify the values of $\\lambda_{etics}$ and $\\alpha_{coating}$\n",
    "\n",
    "## Identification process\n",
    "\n",
    "In an identification process, we try to find the values of unknown parameters\n",
    "that will minimize the gap between a model output and \"the ground truth\" obtained by measurement.\n",
    "\n",
    "We hope that the obtained result correspond to the \"real value\" of the parameters.\n",
    "\n",
    "We use an optimization algorithm to minimise an error function that describe the\n",
    "discrepancy between the model output and the measurement.\n",
    "\n",
    "In this study :\n",
    "- The optimisation algorithm is <code>differential_evolution</code> from\n",
    "<code>scipy.optimize</code> (https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html)\n",
    "- The <code>Identificator</code> error function is the $MSE$ describe in the previous chapter and imported\n",
    "from <code>scikit-learn</code> (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)\n",
    "*Custom error function will be available in future release*\n",
    "\n",
    "## Writing the problem using modelitool\n",
    "\n",
    "### 1- Load the data\n",
    "First we want to load the measured data we will use for the calibration.\n",
    "The dataset will be split in two period, one for the calibration process, and one\n",
    "for the validation.\n",
    "\n",
    "According to the previous chapters, we chose to use 6 consecutive days, from the\n",
    "2018-03-22 to the 2018-03-28. Calibration will be performed over 2 days: 22/03 and 23/03,\n",
    "Validation will be carried on over the remaining period.\n",
    "\n",
    "We use <code>pandas</code> to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"ressources/study_df.csv\",\n",
    "                      parse_dates=True,\n",
    "                      index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Modelica doesn't understand <code>datetime</code> and uses the number of seconds\n",
    "since the beginning of the year as time index. We can use <code>modelitool.combitabconvert.datetime_to_seconds</code>\n",
    "to create a corresponding index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'modelitool'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodelitool\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombitabconvert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime_to_seconds\n\u001b[0;32m      3\u001b[0m time_corr \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(\n\u001b[0;32m      4\u001b[0m     datetime_to_seconds(dataset\u001b[38;5;241m.\u001b[39mindex),\n\u001b[0;32m      5\u001b[0m     index\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mindex\n\u001b[0;32m      6\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'modelitool'"
     ]
    }
   ],
   "source": [
    "from modelitool.combitabconvert import datetime_to_seconds\n",
    "\n",
    "time_corr = pd.Series(\n",
    "    datetime_to_seconds(dataset.index),\n",
    "    index=dataset.index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Load the model\n",
    "We \"import\" the modelica model using a <code>Simulator</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from modelitool.simulate import Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "init_dict = {\n",
    "    \"Twall_init\": 24.81 + 273.15,\n",
    "    \"Tins1_init\": 19.70 + 273.15,\n",
    "    \"Tins2_init\": 10.56 + 273.15,\n",
    "    \"Tcoat_init\": 6.4 + 273.15,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "simulation_opt = {\n",
    "        \"startTime\": time_corr.loc[\"2018-03-22 00:00:00\"],\n",
    "        \"stopTime\": time_corr.loc[\"2018-03-23 23:00:00\"],\n",
    "        \"stepSize\": 300,\n",
    "        \"tolerance\": 1e-06,\n",
    "        \"solver\": \"dassl\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Values in output list correspond to sensors name and value \"T\"\n",
    "simu = Simulator(\n",
    "    model_path=Path(TUTORIAL_DIR) / \"ressources/etics_v0.mo\",\n",
    "    simulation_options=simulation_opt,\n",
    "    init_parameters=init_dict,\n",
    "    output_list=[\"T_ins_ins.T\"],\n",
    "    boundary_df=dataset.loc[\"2018-03-22 00:00:00\":\"2018-03-23 23:00:00\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# These parameters have been identified in a previous study.\n",
    "simu.set_param_dict({\n",
    "    'IR_Amb.Gr': 0.06314703438754983,\n",
    "    'IR_sky.Gr': 0.04675950201241314,\n",
    "    'C_c.C': 7455.526793655418})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Set up parameters\n",
    "Describe the parameters: interval of possible values and initial value.\n",
    "The names must correspond to Modelica syntax\n",
    "\n",
    "In our case we estimate that the parameters can't be more or less than 50% of\n",
    "what we believe to be their \"true value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "id_params = {\n",
    "    'Lambda_ins.k': {\n",
    "        \"init\": 0.04,\n",
    "        \"interval\": (0.04*0.4, 0.04*1.6)\n",
    "    },\n",
    "    'Alpha_clo.k': {\n",
    "        \"init\": 0.5,\n",
    "        \"interval\": (0.2, 0.95)\n",
    "    },\n",
    "    'R_conv_ext.k': {\n",
    "        \"init\": 0.04,\n",
    "        \"interval\": (0.02, 0.2)\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a <code>Identificator</code> to put the problem together.\n",
    "It just requires a <code>Simulator</code> and parameters configuration.\n",
    "If no <code>error_function</code> is specified, the <code>Identificator</code>\n",
    "will use the $MSE$ described in the previous chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from modelitool.identify import Identificator\n",
    "\n",
    "my_identification = Identificator(\n",
    "    simulator=simu,\n",
    "    parameters=id_params,\n",
    "    error_function=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3- Identifications with DE-algorithms\n",
    "\n",
    "The <code>Identificator</code> uses the optimization algorithm to tweak the parameters in order to minimise\n",
    "the deference between the <code>T_coat_ins.T</code> obtained from the model,\n",
    "and the specified reference <code>reference_df[\"T_coat_ins\"]</code>\n",
    "\n",
    "This is done using th <code>fit</code> method, and passing boundary conditions as <code>features</code>\n",
    "and the reference temperature <code>dataset[\"T_coat_ins\"]</code> as <code>labels</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feat_train = dataset.loc[\"2018-03-22 00:00:00\":\"2018-03-23 23:00:00\"]\n",
    "lab_train = dataset.loc[\"2018-03-22 00:00:00\":\"2018-03-23 23:00:00\", [\"T_Ins_Ins\"]] + 273.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feat_val = dataset.loc[\"2018-03-24 00:00:00\":\"2018-03-28 23:55:00\"]\n",
    "lab_val = dataset.loc[\"2018-03-24 00:00:00\":\"2018-03-28 23:55:00\", [\"T_Ins_Ins\"]] + 273.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The possible arguments are:\n",
    "\n",
    "**<code>population_size</code>**, default = 15 : The population has <code>population_size</code> * N (number of parameters) individuals.\n",
    "\n",
    "**<code>convergence_tolerance</code>**, default = 0.05 : Relative tolerance for convergence, the solving stops when np.std(pop) <= atol (0 by default) + tol * np.abs(np.mean(population_energies)).\n",
    "\n",
    "The standard deviation of the energies for each individual in the population, normed by the average, is smaller than the given tolerance value. The tolerance parameters is used to define a stopping condition. The stopping condition is actually that all the individuals (parameter sets) have approximately the same energy, i.e. the same cost function value. Then, the parameter set giving the lowest energy is returned as a solution.\n",
    "\n",
    "It also implies that all the individuals are relatively close to each other in the parameter space. So, no better solution can be expected on the following generations.\n",
    "\n",
    "**<code>crossover_probability</code>**, default = 0.7 : The recombination constant, should be in the range [0, 1]. In the literature this is also known as the crossover probability, being denoted by CR. Increasing this value allows a larger number of mutants to progress into the next generation, but at the risk of population stability *(more info in Storn et Price, 1997)*.\n",
    "\n",
    "**<code>max_iteration</code>**, default = 1000 : The maximum number of generations over which the entire population is evolved. The maximum number of function evaluations is: (<code>max_iteration</code> + 1) * <code>population_size</code> * N\n",
    "\n",
    "\n",
    "***Careful, this step may require a lot of simulation time***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "my_identification.fit(\n",
    "    features=feat_train,\n",
    "    labels=lab_train,\n",
    "#     population_size=15,\n",
    "#     convergence_tolerance=0.01,\n",
    "#     crossover_probability=0.8,\n",
    "#     max_iteration=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Once the optimization process is finished, and if it is considered successful,\n",
    "we can access the values of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(my_identification.param_identified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The results are not satisfying :\n",
    "- The insulation heat conductivity is set to 0.064, the maximum allowed value. This is very unlikely.\n",
    "- the solar radiation heat gain $\\alpha_{coating}$ is set to the maximum allowed value 0.95\n",
    "whereas the true value should be around 0.7.\n",
    "\n",
    "Let's plot the results for the fitting period.\n",
    "We use <code>predict</code> method to get the results for the desired boundary conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "my_identification.param_identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "res_ident_period = my_identification.predict(\n",
    "    features=dataset.loc[\"2018-03-22 00:00:00\":\"2018-03-23 23:00:00\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# figure\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x = res_ident_period.index,\n",
    "    y = res_ident_period[\"T_ins_ins.T\"],\n",
    "    fill=None,\n",
    "    mode='lines',\n",
    "    line_color='brown',\n",
    "    name=\"Model_results\"\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x = lab_train.index,\n",
    "    y = lab_train.squeeze(),\n",
    "    fill='tonexty', # fill area between trace0 and trace1\n",
    "    mode='lines',\n",
    "    line_color='orange',\n",
    "    name=\"Reference_measure\"\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Calibration period Model VS Reality',\n",
    "    xaxis_title='Time training set',\n",
    "    yaxis_title='Coating Temperature [K]')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The results are not accurate :\n",
    "- There seem to be a small \"time shift\" between real temperature peaks and predicted ones\n",
    "- The model do ot reproduce the peaks and the quick temperature variation\n",
    "\n",
    "The algorithm tried to reproduce the high temperatures by maximizing the solar heat gain and the heat transfers\n",
    "through the first layer of insulation, leading to probably false results\n",
    "\n",
    "There could be a problem linked to the model heat capacity that have not been considered here.\n",
    "\n",
    "Let's have a look at the validation period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "res_val_period = my_identification.predict(features=feat_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x = res_val_period.index,\n",
    "    y = res_val_period[\"T_ins_ins.T\"],\n",
    "    fill=None,\n",
    "    mode='lines',\n",
    "    line_color='brown',\n",
    "    name=\"Model_results\"\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x = lab_val.index,\n",
    "    y = lab_val.squeeze(),\n",
    "    fill='tonexty', # fill area between trace0 and trace1\n",
    "    mode='lines',\n",
    "    line_color='orange',\n",
    "    name=\"Reference_measure\"\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Validation period Model VS Reality',\n",
    "    xaxis_title='Time training set',\n",
    "    yaxis_title='Coating Temperature [K]')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The same previous observations apply to the validation period.\n",
    "\n",
    "To better understand the error, we propose two metrics :\n",
    "\n",
    "The Normalized Mean Biased error $NMBE$ :\n",
    "\n",
    "$\n",
    "NMBE = \\frac{1}{\\overline{m}}\n",
    "\\frac\n",
    "    {\\sum \\limits_{i=1}^{N} (s_i - m_i)}\n",
    "    {n}\n",
    "\\times 100\n",
    "$\n",
    "\n",
    "The Coefficient of Variation of Root Mean Square Error $CV(RMSE)$ :\n",
    "\n",
    "$\n",
    "CV(RMSE) =\n",
    "\\frac{1}{\\overline{m}}\n",
    "\\sqrt{\n",
    "    \\frac\n",
    "        {\\sum \\limits_{i=1}^{N} (s_i - m_i)^2}\n",
    "        {n - 1}\n",
    "}\n",
    "\\times 100\n",
    "$\n",
    "\n",
    "With $s$ the simulated values, $m$ the measured values $\\overline{m}$ the mean of measured data.\n",
    "\n",
    "- The metrics are dimensionless. this can be useful to compare the results.\n",
    "A drawback appears if the mean is to close to 0\n",
    "- The $NMBE$ characterize the \"offset\" of the model. Its tendency to over or underestimate the results\n",
    "- The $CV(RMSE)$ characterize the dispersion between prediction and ground truth. The power $^2$ emphasize\n",
    "the large errors.\n",
    "\n",
    "Let's see how the model perform with these 2 metrics for the calibration and validation period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from modelitool.metrics import nmbe\n",
    "from modelitool.metrics import cv_rmse\n",
    "\n",
    "metrics_df = pd.DataFrame(\n",
    "    {\n",
    "        \"NMBE\" : [\n",
    "            nmbe(res_ident_period, lab_train),\n",
    "            nmbe(res_val_period, lab_val)\n",
    "        ],\n",
    "        \"CVRMSE\" : [\n",
    "            cv_rmse(res_ident_period, lab_train),\n",
    "            cv_rmse(res_val_period, lab_val)\n",
    "        ]\n",
    "    },\n",
    "    index=[\"Training\", \"Validation\"]\n",
    ")\n",
    "\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The errors index can be considered as \"pretty\" good. Despite the probably not so good identification of the physical parameter,\n",
    "the model is able to accuratly reproduce the wall thermal behavior.\n",
    "\n",
    "The positive $NMBE$ during both calibration and validation periods confirm that the model have a tendency to overestimate the\n",
    "insulation temperature\n",
    "\n",
    "we proppose to perform another calibration using an error function that uses $NMBE$ and $CV(RMSE)$ :\n",
    "\n",
    "$\n",
    "error = abs(NMBE) + CV(RMSE)\n",
    "$\n",
    "\n",
    "Using the absolute value of $NMBE$ will make the optimization harder to solve. But since the metrics can be negative,\n",
    "False results will occur when added with $CV(RMSE)$.\n",
    "\n",
    "_Note that we could add weight to each metric to emphasize their effect_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def combined_error(y_pred, y_true):\n",
    "    return abs(nmbe(y_pred,y_true)) + cv_rmse(y_pred,y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "my_combined_identification = Identificator(\n",
    "    simulator=simu,\n",
    "    parameters=id_params,\n",
    "    error_function=combined_error,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "my_combined_identification.fit(features=feat_val, labels=lab_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "my_combined_identification.param_identified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Oups, the obtained results are completly different.\n",
    "- $\\lambda_{etics}$ is still set to its maximum value\n",
    "- $\\alpha_{coating}$ is low and minimize solar heat gain\n",
    "- $R_{conv}$ is set to the maximum probably to compensate the lower solar gain\n",
    "\n",
    "On a physical point of view, these results are a bit less unlikely.\n",
    "But it is probably also wrong.\n",
    "Let's compare the two models prediction over the full measurement period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "res_full_custom = my_combined_identification.predict(features=dataset)\n",
    "res_full = my_identification.predict(features=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# figure\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x = res_full.index,\n",
    "    y = res_full[\"T_ins_ins.T\"],\n",
    "    fill=None,\n",
    "    mode='lines',\n",
    "    line_color='brown',\n",
    "    name=\"Model_mse_minimise\"\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x = res_full_custom.index,\n",
    "    y = res_full_custom[\"T_ins_ins.T\"],\n",
    "    fill=None,\n",
    "    mode='lines',\n",
    "    line_color='violet',\n",
    "    name=\"Model_custom_minimise\"\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x = dataset.index,\n",
    "    y = dataset.T_Ins_Ins.squeeze() + 273.15,\n",
    "    fill='tonexty', # fill area between trace0 and trace1\n",
    "    mode='lines',\n",
    "    line_color='orange',\n",
    "    name=\"Reference_measure\"\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Calibration period Model VS Reality',\n",
    "    xaxis_title='Time training set',\n",
    "    yaxis_title='Coating Temperature [K]')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "metrics_df = pd.DataFrame(\n",
    "    {\n",
    "        \"NMBE\" : [\n",
    "            nmbe(res_full.squeeze(), dataset.T_Ins_Ins + 273.15),\n",
    "            nmbe(res_full_custom.squeeze(), dataset.T_Ins_Ins + 273.15)\n",
    "        ],\n",
    "        \"CVRMSE\" : [\n",
    "            cv_rmse(res_full.squeeze(), dataset.T_Ins_Ins + 273.15),\n",
    "            cv_rmse(res_full_custom.squeeze(), dataset.T_Ins_Ins + 273.15)\n",
    "        ],\n",
    "        \"MSE\" : [\n",
    "            mean_squared_error(res_full.squeeze(), dataset.T_Ins_Ins + 273.15),\n",
    "            mean_squared_error(res_full_custom.squeeze(), dataset.T_Ins_Ins + 273.15)\n",
    "        ]\n",
    "    },\n",
    "    index=[\"MSE_fitting\", \"Custom_fitting\"]\n",
    ")\n",
    "\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First, the metrics indicate that the calibration process performed well\n",
    "The new model is better \"centered\" at the expense of the squared error.\n",
    "\n",
    "From these information, it is still difficult to draw valid conclusion on the paramters values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Model identification is a powerfull tool to obtain material or system properties using measurement\n",
    "and model.\n",
    "\n",
    "However, we must keep in mind that a \"false\" model will provide wrong results.\n",
    "\n",
    "In this case, we succeeded at creating a model that more or less reproduce the wall thermal behavior.\n",
    "Unfortunatly we cannot draw any conclusion on the wall material properties. That's a shame as it was the whole point\n",
    "of the experiment.\n",
    "\n",
    "Several imporvement can be made to this test bed :\n",
    "- Perform additionnal measurement such as heat flux\n",
    "- Provide a more accurate model (better description of phenomenon). But be carefull, this may required more accurate measure such as wind speed and direction.\n",
    "\n",
    "Finally other identification method (and much more complex and computationnaly intensive) may be used such as stochastic method based on Baye's theorem.\n",
    "This is the point of the next Chapter __Bayesian Methods for model identification__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
